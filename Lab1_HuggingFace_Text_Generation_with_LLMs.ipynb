{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/MA_AI900/blob/main/Lab1_HuggingFace_Text_Generation_with_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8UcmTSbkUC_"
      },
      "source": [
        "# Experimenting with Open Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKB3HJRqq_M8"
      },
      "source": [
        "## HuggingFace: Using Pre-trained LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQUotuMgYaMb"
      },
      "source": [
        "### LLM: GPT-2\n",
        "\n",
        "Context: https://huggingface.co/openai-community/gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ggocMJ4_Ypx"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='openai-community/gpt2')\n",
        "generator(\"Hello, I'm a language model,\", max_length=30)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1QRBBpwYfdM"
      },
      "source": [
        "### Microsoft Phi2\n",
        "\n",
        "\n",
        "Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.\n",
        "\n",
        "\n",
        "HF: https://huggingface.co/microsoft/phi-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3d982bd"
      },
      "source": [
        "**Key Features:**\n",
        "\n",
        "*   **Parameters:** 2.7 billion\n",
        "*   **Architecture:** Transformer\n",
        "*   **Training Data:** Same sources as Phi-1.5, plus new NLP synthetic texts and filtered websites.\n",
        "*   **Performance:** Near state-of-the-art among models under 13 billion parameters on common sense, language understanding, and logical reasoning benchmarks.\n",
        "*   **Input Modality:** Text\n",
        "*   **Output Modality:** Text\n",
        "*   **Input Context Window Size:** Not explicitly stated.\n",
        "*   **Output Context Window Size:** Not explicitly stated.\n",
        "*   **Organization:** Microsoft\n",
        "\n",
        "**Use Cases:**\n",
        "\n",
        "*   Text generation tasks.\n",
        "*   Applications requiring strong common sense, language understanding, and logical reasoning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "pipe = pipeline(\"text-generation\", model=\"microsoft/phi-2\", device=device)"
      ],
      "metadata": {
        "id": "AfJfDBOpmG9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"Explain quantum computing in simple terms.,\", max_length=30, num_return_sequences=1)"
      ],
      "metadata": {
        "id": "BKR9OnUmniUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3c96a28"
      },
      "source": [
        "### Qwen/Qwen3-0.6B\n",
        "\n",
        "Qwen3-0.6B is a 0.6 billion parameter language model from the Qwen family of models. These models are known for their strong performance across various benchmarks.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "*   **Parameters:** 0.6 billion\n",
        "*   **Input Modality:** Text\n",
        "*   **Output Modality:** Text\n",
        "*   **Input Context Window Size:** Not explicitly stated, but typically large for Qwen models.\n",
        "*   **Output Context Window Size:** Not explicitly stated.\n",
        "*   **Training Data:** Trained on a large corpus of text and code data.\n",
        "*   **Organization:** Qwen\n",
        "\n",
        "**Use Cases:**\n",
        "\n",
        "*   Text generation tasks.\n",
        "*   Applications where a smaller, efficient model is suitable.\n",
        "\n",
        "**Hugging Face:** [https://huggingface.co/Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ8-6yfnzTpJ"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen3-0.6B\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4h_P6m10jfl"
      },
      "source": [
        "## Multimodal LLMs\n",
        "\n",
        "### Microsoft Kosmos-2.5\n",
        "\n",
        "Kosmos-2.5 is a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCmLWhe8BBdR"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or9oODTVk-zO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import requests\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "repo = \"microsoft/kosmos-2.5-chat\"\n",
        "device = \"cuda:0\"\n",
        "dtype = torch.bfloat16\n",
        "\n",
        "# sample image\n",
        "url = \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU0KyyVTzpcQ"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"microsoft/kosmos-2.5-chat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adWLmeMuBU8G"
      },
      "outputs": [],
      "source": [
        "question = \"What is the sub total of the receipt?\"\n",
        "template = \" USER: {} ASSISTANT:\"\n",
        "prompt = template.format(question)\n",
        "\n",
        "\n",
        "generated_text = pipe(images=image, text=prompt, max_new_tokens=1024)\n",
        "generated_text[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtw9yI_Z-MQ7"
      },
      "source": [
        "## Thank You"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}